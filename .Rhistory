par(mfrow=c(1,1))
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1])
boot_CI[1,]
mod_CI[1,]
plot_CIs <- function(CI_boot,mean_boot, CI_mod, mean_mod, title="Comparing confidence intervals"){
#
# Dockstring
#
# Example data
boot_lower <- CI_boot[1]; boot_upper <- CI_boot[2]; boot_mean <- mean_boot
mod_lower <- CI_mod[1]; mod_upper <- CI_mod[2]; mod_mean <- mean_mod
# Set up the plot
plot(1, type="n", xlim=c(-2.5, 1.5), ylim=c(0, 3), xlab="", ylab="", yaxt="n", main=title, axes=TRUE)
# Add horizontal lines for the CIs
segments(x0=boot_lower, y0=1, x1=boot_upper, y1=1, lwd=2, col="blue")
segments(x0=mod_lower, y0=2, x1=mod_upper, y1=2, lwd=2, col="red")
# Add vertical lines for the CI limits
segments(x0=boot_lower, y0=0.9, x1=boot_lower, y1=1.1, lwd=2, col="blue")
segments(x0=boot_upper, y0=0.9, x1=boot_upper, y1=1.1, lwd=2, col="blue")
segments(x0=mod_lower, y0=1.9, x1=mod_lower, y1=2.1, lwd=2, col="red")
segments(x0=mod_upper, y0=1.9, x1=mod_upper, y1=2.1, lwd=2, col="red")
# Add points for the means
points(x=mean_boot, y=1, pch=19, col="blue")
points(x=mean_mod, y=2, pch=19, col="red")
# Optionally, add a legend
legend("topright", legend=c("Bootstrap", "MLE"), col=c("blue", "red"), lty=1, pch=19)
}
par(mfrow=c(1,1))
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1])
boot_CI[1,]
mod_CI[1,]
plot_CIs <- function(CI_boot,mean_boot, CI_mod, mean_mod, title=""){
#
# Dockstring
#
# Example data
boot_lower <- CI_boot[1]; boot_upper <- CI_boot[2]; boot_mean <- mean_boot
mod_lower <- CI_mod[1]; mod_upper <- CI_mod[2]; mod_mean <- mean_mod
# Set up the plot
plot(1, type="n", xlim=c(-2.5, 1.5), ylim=c(0, 3), xlab="", ylab="", yaxt="n", main=title, axes=TRUE)
# Add horizontal lines for the CIs
segments(x0=boot_lower, y0=1, x1=boot_upper, y1=1, lwd=2, col="blue")
segments(x0=mod_lower, y0=2, x1=mod_upper, y1=2, lwd=2, col="red")
# Add vertical lines for the CI limits
segments(x0=boot_lower, y0=0.9, x1=boot_lower, y1=1.1, lwd=2, col="blue")
segments(x0=boot_upper, y0=0.9, x1=boot_upper, y1=1.1, lwd=2, col="blue")
segments(x0=mod_lower, y0=1.9, x1=mod_lower, y1=2.1, lwd=2, col="red")
segments(x0=mod_upper, y0=1.9, x1=mod_upper, y1=2.1, lwd=2, col="red")
# Add points for the means
points(x=mean_boot, y=1, pch=19, col="blue")
points(x=mean_mod, y=2, pch=19, col="red")
# Optionally, add a legend
legend("topright", legend=c("Bootstrap", "MLE"), col=c("blue", "red"), lty=1, pch=19)
}
par(mfrow=c(1,1))
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1])
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1], title = "Beta0")
plot_CIs(boot_CI[2,], mean(b$beta1), mod_CI[2,], mod$coefficients[2], title = "Beta1")
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1], title = "Beta0")
plot_CIs <- function(CI_boot,mean_boot, CI_mod, mean_mod, title="", xlim=c(-2,2)){
#
# Dockstring
#
# Example data
boot_lower <- CI_boot[1]; boot_upper <- CI_boot[2]; boot_mean <- mean_boot
mod_lower <- CI_mod[1]; mod_upper <- CI_mod[2]; mod_mean <- mean_mod
# Set up the plot
plot(1, type="n", xlim=c(xlim[1], xlim[2]), ylim=c(0, 3), xlab="", ylab="", yaxt="n", main=title, axes=TRUE)
# Add horizontal lines for the CIs
segments(x0=boot_lower, y0=1, x1=boot_upper, y1=1, lwd=2, col="blue")
segments(x0=mod_lower, y0=2, x1=mod_upper, y1=2, lwd=2, col="red")
# Add vertical lines for the CI limits
segments(x0=boot_lower, y0=0.9, x1=boot_lower, y1=1.1, lwd=2, col="blue")
segments(x0=boot_upper, y0=0.9, x1=boot_upper, y1=1.1, lwd=2, col="blue")
segments(x0=mod_lower, y0=1.9, x1=mod_lower, y1=2.1, lwd=2, col="red")
segments(x0=mod_upper, y0=1.9, x1=mod_upper, y1=2.1, lwd=2, col="red")
# Add points for the means
points(x=mean_boot, y=1, pch=19, col="blue")
points(x=mean_mod, y=2, pch=19, col="red")
# Optionally, add a legend
legend("topright", legend=c("Bootstrap", "MLE"), col=c("blue", "red"), lty=1, pch=19)
}
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1], title = "Beta0", xlim=c(-2.5,1.5))
plot_CIs(boot_CI[2,], mean(b$beta1), mod_CI[2,], mod$coefficients[2], title = "Beta1", xlim=c(-2.5,1.5))
plot_CIs(boot_CI[2,], mean(b$beta1), mod_CI[2,], mod$coefficients[2], title = "Beta1", xlim=c(-1,.5))
plot_CIs(boot_CI[2,], mean(b$beta1), mod_CI[2,], mod$coefficients[2], title = "Beta1", xlim=c(-.3,.5))
par(mfrow=c(1,2))
plot_CIs(boot_CI[1,], mean(b$beta0), mod_CI[1,], mod$coefficients[1], title = "Beta0", xlim=c(-2.5,1.5))
plot_CIs(boot_CI[2,], mean(b$beta1), mod_CI[2,], mod$coefficients[2], title = "Beta1", xlim=c(-.3,.5))
boot_CI[1,]
mod_CI[1,]
boot_CI
mod_CI
# Include libraries
library(ggplot2)
library(MASS)
library(dplyr)
library(latex2exp)
set.seed(1) # For reproducibility
# Load data
load(file=url("https://www.math.ntnu.no/emner/TMA4300/2024v/data.Rdata"))
data
# Fit glm
mod <- glm(cbind(y, m - y) ~ x, family=binomial, data=data)
mod
1-pchisq(13.24,8)
anova(mod,test = "Chisq")
anova(mod,test = "LRT")
mod
summary(mod)
# Include libraries
library(ggplot2)
library(MASS)
library(dplyr)
library(latex2exp)
set.seed(1) # For reproducibility
5-3
3-(5-3)
# Include libraries
library(ggplot2)
library(MASS)
library(dplyr)
library(latex2exp)
set.seed(1) # For reproducibility
# Load data
load(file=url("https://www.math.ntnu.no/emner/TMA4300/2024v/data.Rdata"))
data
# Fit glm
mod <- glm(cbind(y, m - y) ~ x, family=binomial, data=data)
beta_boot <- function(model, B = 10000){
#
# Dockstring
#
n <- dim(mod$data)[1]
idx <- sample(1:n, n*B, replace = TRUE)
boot <- data.frame(data[idx,], ii = rep(1:B)) %>% group_by(ii)
beta0 <- c()
beta1 <- c()
for (i in 1:B){
tempdata <- boot[boot$ii == i,]
tempmod <- glm(cbind(y, m-y) ~ x, family = binomial, data = tempdata)
beta0 <- c(beta0, tempmod$coefficients[1])
beta1 <- c(beta1, tempmod$coefficients[2])
}
betas <- data.frame(beta0 = beta0, beta1 = beta1)
return(betas)
}
np_boot <- beta_boot(mod)
vcov(mod)
var(np_boot)
print("Bootstrap estimaes:")
sapply(np_boot,mean)
print("")
print("ML estimates:")
coef(mod)
par(mfrow=c(1,2))
hist(np_boot$beta0, breaks = 20, main = "Distribution of bootstrapped beta0", xlab = "beta0", cex.main=1)
abline(v=mean(np_boot$beta0), col="red",lwd=2)
abline(v=coef(mod)[1],lwd=2)
legend("topright",legend=c("Boot", "MLE"),
col=c("red","black"), lty = 1,cex = 1, bg="lightblue")
hist(np_boot$beta1, breaks = 20, main = "Distribution of bootstrapped beta1", xlab = "beta1", cex.main=1)
abline(v=mean(np_boot$beta1), col="red", lwd=2)
abline(v=coef(mod)[2], lwd=2)
legend("topleft",legend=c("Boot", "MLE"),
col=c("red","black"), lty = 1,cex = 1, bg="lightblue")
b1_bias <- mean(np_boot$beta0-mod$coefficients[1])
b2_bias <- mean(np_boot$beta1-mod$coefficients[2])
b1_corrected <- coef(mod)[1] + b1_bias
b2_corrected <- coef(mod)[2] + b2_bias
#Booth by computation and from visual representation, it does not seem like the MLEs of the coefs are
# significantly biased.
b1_bias <- mean(np_boot$beta0-mod$coefficients[1])
b2_bias <- mean(np_boot$beta1-mod$coefficients[2])
b1_corrected <- coef(mod)[1] - b1_bias
b2_corrected <- coef(mod)[2] - b2_bias
#Booth by computation and from visual representation, it does not seem like the MLEs of the coefs are
# significantly biased.
sapply(np_boot,mean)
b1_corrected
b1_bias <- mean(np_boot$beta0-mod$coefficients[1])
b2_bias <- mean(np_boot$beta1-mod$coefficients[2])
b1_corrected <- coef(mod)[1] - b1_bias
b2_corrected <- coef(mod)[2] - b2_bias
#Booth by computation and from visual representation, it does not seem like the MLEs of the coefs are
# significantly biased.
mod$coefficients
b1_bias <- mean(np_boot$beta0-mod$coefficients[1])
b1_corrected <- coef(mod)[1] - b1_bias
b1_corrected
2*coef(mod)[1]
coef(mod)[1]
2*coef(mod)[1]
sapply(np_boot,mean)[1]
2*coef(mod)[1]-sapply(np_boot,mean)
2*coef(mod)[1]-sapply(np_boot,mean)[1]
b1_corrected <- coef(mod)[1] + b1_bias
b1_corrected
2*4-5
summary(mod)
plot(mod$data$x,mod$data$y)
plot(mod$data$x,mod$data$y, lty=1)
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y, lty=1)
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y, col="black")
plot(mod$data$x,mod$data$y, col="black",pch=16)
plot(mod$data$x,mod$data$y, col="black",pch=26)
plot(mod$data$x,mod$data$y, col="black",pch=6)
plot(mod$data$x,mod$data$y, col="red",pch=16)
plot(mod$data$x,mod$data$y, col="black",pch=16)
plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe")
plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x")
plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values, col="black",pch=16)
points(mod$data$x,mod$fitted.values, col="red",pch=16)
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values, col="red",pch=16)
par(mfrow=c(1,1))
#plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values, col="red",pch=16)
legennd
par(mfrow=c(1,1))
#plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values, col="red",pch=16)
par(mfrow=c(1,1))
#plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values, col="red",pch=16)
#plot(mod$data$x,mod$data$y, col="black",pch=16,main = "aekfe",xlab="x",ylab = "y")
plot(mod$data$x,mod$fitted.values, col="red",pch=16)
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y,pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values,pch=16)
fitted.values(mod)
plot(mod$data$x,mod$data$y/mod$data$x,pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values,pch=16)
plot(mod$data$x,mod$data$y/10,pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values,pch=16)
legend("bottomright",legend = c("2eea","afefa"), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,pch=16,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values,pch=16)
legend("bottomright",legend = c("2eea","afefa"), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values)
legend("bottomright",legend = c("2eea","afefa"), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y")
points(mod$data$x,mod$fitted.values)
legend("bottomright",legend = c("2eea","afefa"), pch=c(19,19), col=c("black","red"))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black")
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black")
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19)
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19)
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend("bottomright",legend = c("2eea","afefa"), pch=c(19,19), col=c("black","red"))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend("bottomright",legend = c("2eea","afefa"), pch=c(19,19), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend(1,9,legend = c("2eea","afefa"), pch=c(19,19), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend(10,9,legend = c("2eea","afefa"), pch=c(19,19), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend("bottomright",legend = c("2eea","afefa"), pch=c(19,19), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend("bottomright",legend = c("Predictions","Data"), pch=c(19,19), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x,mod$data$y/10,main = "aekfe",xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend("bottomright",legend = c("Data","Predictions"), pch=c(19,19), col=c("black","red"))
par(mfrow=c(1,1))
plot(mod$data$x, mod$data$y/10, main = "Scatter of fitted values vs. actual data",
xlab="x",ylab = "y",col="black",pch=19, ylim = c(0,1))
points(mod$data$x,mod$fitted.values, col="red",pch=19)
legend("bottomright",legend = c("Data","Predictions"),
pch=c(19,19), col=c("black","red"))
confint(mod, level = 80)
mod
confint(mod, level = 0.8)
confint(mod, level = 0.9)
confint(mod, level = 0.85)
confint(mod, level = 0.8)
mod
qchisq(0.025)
qchisq(0.025,2)
?qchisq
?qgamma
alpha <- 0.05
n_values <- c(5, 10, 20, 50, 100)
coverage_probabilities <- sapply(n_values, function(n) {
chi_low <- qchisq(alpha / 2, 2 * n)
chi_high <- qchisq(1 - alpha / 2, 2 * n)
cdf_high <- pchisq(chi_high, 2 * n)
cdf_low <- pchisq(chi_low, 2 * n)
coverage_probability <- cdf_high - cdf_low
return(coverage_probability)
})
names(coverage_probabilities) <- paste("Coverage for n =", n_values)
coverage_probabilities
qchisq(alpha/2,10)
qchisq(alpha/2,100)
lower <- qchisq(alpha/2,10)
upper <- qchisq(1-alpha/2,10)
pchisq(upper,10)
pchisq(lowe,10)
pchisq(lower,10)
lower
upper
pchisq(20/upper,10)
pchisq(20/lower,10)
pchisq(upper/20,10)
pchisq(lower/20,10)
pchisq(5,2*4)
pchisq(5,4)
require(ggplot2)
require(dplyr)
x <- arcsin(3)
x <- asin(3)
x <- asin(pi)
x
x <- asin(sin(pi))
x
u <- runif(length(x))
x <- asin(2*u-1)
hist(x)
n <- 10000
u <- runif(n)
x <- asin(2*u-1)
hist(x)
3+3
log(1)
log(2*0.5)
log(2*0.4)
log(1-2*0.5)
log(1-2*0.6)
log(1-2*0.4)
log(2*0.4-1)
log(2*0.5-1)
log(2*0.6-1)
log(2*0.7-1)
log(2*0.7-1)
-log(2*0.7-1)
n <- 10000
u <- runif(n)
x <- log(2*u)[u<0.5]
hist(x)
x2 <- log(2u-1)[u>0.5]
x2 <- log(2*u-1)[u>0.5]
hist(x2)
x2 <- -log(2*u-1)[u>0.5]
hist(x2)
ln(2)
log(2)
log(1.1)
x2 <- -log(2-2*u)[u>0.5]
hist(x2)
n <- 10000
u <- runif(n)
x <- log(2*u)[u<0.5]
x2 <- -log(2-2*u)[u>0.5]
hist(x2)
sqrt(pi/2)
sqrt(pi/2)*exp(-1/2)
1**0.5
updateR()
require(installr)
install.packages("installr")
require(installr)
updateR()
require(ggplot2)
install.packages("lme4")
library(lme4)
setwd("C:/Users/simen/OneDrive - NTNU/Documents/Potential ProjectMaster/ProjectThesis")
#######################################################################
## Stefanie Muff, September 2023
##
## Helgeland house sparrow data analysis
## Make predictions using Machine Learning (deep learning)
## In addition, at the end we give code for the genomics-based animal model fitted with INLA
## !! This requires that you are using R version 4.2 or higher (ideally even 4.3) !!
#######################################################################
# Packages needed for the script to run:
library(nadiv)
library(pedigree)
library(INLA)
library(MASS)
library(MCMCpack)
library(MCMCglmm)
# This is a self-made package that I send you to install locally:
library(SMisc)
library(dplyr)
library(keras)
library(tensorflow)
# Data preparation helper script:
source("h_dataPrep.r")
# Some data wranging to ensure that the IDs in the data correspond to the IDs in the A and G-matrices (nothing to worry about):
# indicates that some IDs are missing:
d.map[3110:3125,]
# from this we see the number of anmals
Nanimals <- 3116
phenotype <- "mass"
# remove missing values
d.morph <- filter(d.morph, !is.na(eval(as.symbol(phenotype))))
# In the reduced pedigree only Nanimals out of the 3147 IDs are preset.
d.map$IDC <- 1:nrow(d.map)
d.morph$IDC <- d.map[match(d.morph$ringnr, d.map$ringnr), "IDC"]
### Prepare for use in INLA -
d.morph$IDC4 <- d.morph$IDC3 <- d.morph$IDC2 <- d.morph$IDC
# Relatedness matrix from Henrik (vanRaden method 1) where +0.01 was already added to diagnoal!
d.Gmatrix <- read.table("data/gghatvr3.triangle.g", header = F, sep=" ")
# G is a sparse matrix object. We can also verify that it is symmetric (sometimes some numerical problems lead to non-symmetry)
G <- sparseMatrix(i=d.Gmatrix[,1],j=d.Gmatrix[,2],x=d.Gmatrix[,3],symmetric=T)
G[,] <- as.numeric(G[,])
isSymmetric(G)
# Again extract the rows and columns for the individuals in the data set that we analyse
GG <- G[d.map[1:3116,3],d.map[1:3116,3]]
# To ensure that the matrix is positive definite, we do a computational trick (proposed by vanRaden 2008, see https://doi.org/10.3168/jds.2007-0980 :)
GGG  <- GG*0.99 + 0.01
# Need to derive the inverse to give to INLA
Cmatrix <- solve(GGG)
if (!isSymmetric(Cmatrix)){
Cmatrix <- forceSymmetric(Cmatrix)
}
##
## INLA formula
##
#Here we use body mass as the response, and some fixed and random effects:
formula.mass = eval(as.symbol(phenotype)) ~ sex + FGRM + month + age +   outer + other +
f(hatchisland, model = "iid", hyper = list(
prec= list(initial = log(1), prior = "pc.prec", param = c(1,0.05))
)) +
f(hatchyear,model="iid",hyper=list(
prec=list(initial=log(1), prior="pc.prec",param=c(1,0.05))
))+
f(IDC,model="iid",hyper=list(
prec=list(initial=log(1), prior="pc.prec",param=c(1,0.05))
)) +
f(IDC2,values=1:3116,model="generic0",
Cmatrix=Cmatrix,
constr = TRUE,
hyper=list(
# The priors are relevant, need to discuss
prec=list(initial=log(0.5), prior="pc.prec",param=c(sqrt(2),0.05))
))
fold_data <- data.frame(ringnr = character(), fold = integer())
for (i in 1:10){
data_from_file <- read.csv(paste("data/interim/random2_10fold_",i,".csv",sep = ""))
temp_df <- data.frame(ringnr = data_from_file$ringnr, fold = i)
#append to the fold_data
fold_data <- rbind(fold_data, temp_df)
}
fold_data
corr_cv_EG <- c()
corr_cv <- c()
########################################################
#Everthing under here must go in a loop later
########################################################
for (i in 2:10){
ringnr_test <- fold_data[fold_data$fold == i, "ringnr"]
ringnr_train <- fold_data[fold_data$fold !=i, "ringnr"]
#make train and test split
d.morph_train <- filter(d.morph, !ringnr %in% ringnr_test)
d.morph_test <- filter(d.morph, ringnr %in% ringnr_test)
n_train <- dim(d.morph_train)[1]
n_test <- dim(d.morph_test)[1]
N <- n_train + n_test
#Saving the phenotypic value in the test set, both for phenotype and breeding value
pheno_test_EG <- d.morph_test[,phenotype]
pheno_test <- as.data.frame(d.morph_test %>%
group_by(ringnr) %>%
summarize(
mean_pheno = mean(eval(as.symbol(phenotype)))
))[,"mean_pheno"]
#INLA does not have a predict function, so have to fill the test-values with NAs and merge it back into the training set.
d.morph_test[, phenotype] <- NA
d.morph_train <- union_all(d.morph_train, d.morph_test)
names(d.morph_train)
#All individuals
idxs <- 1:Nanimals
#get the indices corresponding to the individuals in the test set.
idxs_test <- which(d.map$ringnr %in% unique(ringnr_test))
cat("STARTING INLA\n")
model1.mass = inla(
formula=formula.mass,
family="gaussian",
data=d.morph_train,
control.family=list(hyper = list(theta = list(initial=log(0.5),  prior="pc.prec",param=c(sqrt(2),0.05)))),
control.compute=list(dic=F, return.marginals=FALSE), verbose = TRUE
)
cat("INLA DONE")
#get predicted values
preds_EG <- model1.mass$summary.fitted.values$mean[(n_train + 1):N]
preds <- model1.mass$summary.random$IDC2$mode[idxs_test]
#calculate matrics:
corr_EG <- cor(preds_EG, pheno_test_EG, method = "pearson")
corr <- cor(preds, pheno_test, method = "pearson")
new_data <- data.frame(corr_EG, corr)
write.table(new_data, "INLA_results2.csv", col.names = FALSE, row.names = FALSE, append = TRUE)
#write.csv(data.frame(corr_EG, corr), "INLA_results2.csv", row.names = FALSE)
}
